{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Notebook setup\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = InteractiveSession(config=config)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set random seed to create reproducable results\n",
    "the_seed = 1234\n",
    "np.random.seed(the_seed)\n",
    "random.seed(the_seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(the_seed)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "We use the biosbias setting as an example to show how to load and run a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_PATH = f\"GLoVe/glove.6B.{EMBEDDING_DIM}d.txt\" # Path to your glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'EMNLP2020-Data2Share/Data2Share/'\n",
    "MAIN_DATASET = 'YelpSmall500'\n",
    "SECOND_DATASET = None\n",
    "THIRD_DATASET = None\n",
    "GENDER_BIAS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Please change MODEL_FILE_PATH to the path of a pre-trained model you want to run in your local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE_PATH = 'Yelp-AmazonProducts-CNNs/1A/YelpSmall500_CNN_20200515014923.h5'\n",
    "MODEL_ARCH = 'CNN'\n",
    "MAXLEN = 150\n",
    "FILTERS = [(10, 2), (10, 3), (10, 4)] # Ten filters of each window size [2,3,4]\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embeddings and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14524.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# 0. Load GloVe embeddings\n",
    "embedding_matrix, vocab_size, index2word, word2index = find.get_embedding_matrix(EMBEDDING_PATH, EMBEDDING_DIM, pad_initialisation = \"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1669.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2344.72it/s]\n",
      "100%|██████████| 38000/38000 [00:15<00:00, 2516.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load datasets and prepare inputs\n",
    "# 1.1 Main dataset\n",
    "data_1 = pickle.load(open(DATA_PATH + f'all_data_{MAIN_DATASET}.pickle', 'rb'))\n",
    "class_names = data_1['class_names']\n",
    "X_train_1, X_validate_1, X_test_1 = find.get_data_matrix(data_1['text_train'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_validate'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_test'], word2index, MAXLEN)\n",
    "y_test_1 = data_1['y_test']\n",
    "#gender_test_1 = data_1['gender_test'] if GENDER_BIAS else None\n",
    "\n",
    "# 1.2 Second dataset\n",
    "if SECOND_DATASET is not None:\n",
    "    data_2 = pickle.load(open(DATA_PATH + f'all_data_{SECOND_DATASET}.pickle', 'rb'))\n",
    "    X_test_2, y_test_2 = find.get_data_matrix(data_2['text_test'], word2index, MAXLEN), data_2['y_test']\n",
    "    gender_test_2 = data_2['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_2, y_test_2, gender_test_2 = None, None, None\n",
    "\n",
    "# 1.3 Third dataset\n",
    "if THIRD_DATASET is not None:\n",
    "    data_3 = pickle.load(open(DATA_PATH + f'all_data_{THIRD_DATASET}.pickle', 'rb'))\n",
    "    X_test_3, y_test_3 = find.get_data_matrix(data_3['text_test'], word2index, MAXLEN), data_3['y_test']\n",
    "    gender_test_3 = data_3['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_3, y_test_3, gender_test_2  = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, load, and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     120000600   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 10)      6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 148, 10)      9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 147, 10)      12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_1 (MaskedDense)    (None, 2)            122         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 27,092\n",
      "Non-trainable params: 120,000,660\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a model\n",
    "if MODEL_ARCH == 'CNN':\n",
    "    model = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, FILTERS)\n",
    "else:\n",
    "    assert False, f\"Unsupported model architecture: {MODEL_ARCH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load pre-trained model weights\n",
    "model.load_weights(MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "{'per_class': {0: {'all_positive': 25161,\n",
      "                   'all_true': 19000,\n",
      "                   'class_f1': 0.7917846063268495,\n",
      "                   'class_name': 'Negative',\n",
      "                   'class_precision': 0.6948451969317595,\n",
      "                   'class_recall': 0.9201578947368421,\n",
      "                   'true_positive': 17483},\n",
      "               1: {'all_positive': 12839,\n",
      "                   'all_true': 19000,\n",
      "                   'class_f1': 0.71120324130783,\n",
      "                   'class_name': 'Positive',\n",
      "                   'class_precision': 0.8818443804034583,\n",
      "                   'class_recall': 0.5958947368421053,\n",
      "                   'true_positive': 11322}},\n",
      " 'total': {'accuracy': 0.7580263157894737,\n",
      "           'macro_f1': 0.7728883370920798,\n",
      "           'macro_precision': 0.7883447886676089,\n",
      "           'macro_recall': 0.7580263157894737,\n",
      "           'micro_f1': 0.7580263157894737,\n",
      "           'micro_precision': 0.7580263157894737,\n",
      "           'micro_recall': 0.7580263157894737}}\n",
      "FPR = 0.0798421052631579\n",
      "FNR = 0.40410526315789475\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = None, model_name = 'original')\n",
    "else:\n",
    "    find.evaluate_all(model, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, gender_test_2, result_path = None, model_name = 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}