{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Biosbias\n",
    "- **Task**: For Biosbias, the task is predicting the occupation of a given bio paragraph, i.e., whether the person is 'a surgeon' (class 0) or 'a nurse' (class 1).\n",
    "- **Problem**: Due to the gender imbalance in each occupation, a classifier usually exploits gender information when making predictions. As a result, bios of female surgeons and male nurses are often misclassified. We quantify the bias of the model using two metrics: **FPED and FNED** (For details, please see [Dixon et al., 2018](https://dl.acm.org/doi/pdf/10.1145/3278721.3278729)). \n",
    "- **Solution**: To reduce the model's bias, we use our framework to identify the features which detect gender information rather than occupation and disable such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Notebook setup\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = InteractiveSession(config=config)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set random seed to create reproducable results\n",
    "the_seed = 1234\n",
    "np.random.seed(the_seed)\n",
    "random.seed(the_seed)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(the_seed)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe word embeddings: Please replace the string in the second line with a path to your GloVe embeddings file which can be download [here](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_PATH = f\"GLoVe/glove.6B.{EMBEDDING_DIM}d.txt\" # Path to your glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'preprocessed_data/'\n",
    "MAIN_DATASET = 'Biosbias2'\n",
    "SECOND_DATASET = None\n",
    "THIRD_DATASET = None\n",
    "GENDER_BIAS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'trained_models/'\n",
    "MODEL_ARCH = 'CNN'\n",
    "MAXLEN = 150\n",
    "FILTERS = [(10, 2), (10, 3), (10, 4)] # Ten filters of each window size [2,3,4]\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:28, 13845.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# 0. Load GloVe embeddings\n",
    "embedding_matrix, vocab_size, index2word, word2index = find.get_embedding_matrix(EMBEDDING_PATH, EMBEDDING_DIM, pad_initialisation = \"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3832/3832 [00:01<00:00, 3520.74it/s]\n",
      "100%|██████████| 1277/1277 [00:00<00:00, 3828.27it/s]\n",
      "100%|██████████| 1278/1278 [00:00<00:00, 4087.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load datasets and prepare inputs\n",
    "# 1.1 Main dataset\n",
    "data_1 = pickle.load(open(DATA_PATH + f'all_data_{MAIN_DATASET}.pickle', 'rb'))\n",
    "class_names = data_1['class_names']\n",
    "X_train_1, X_validate_1, X_test_1 = find.get_data_matrix(data_1['text_train'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_validate'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_test'], word2index, MAXLEN)\n",
    "y_test_1 = data_1['y_test']\n",
    "gender_test_1 = data_1['gender_test'] if GENDER_BIAS else None\n",
    "\n",
    "# 1.2 Second dataset\n",
    "if SECOND_DATASET is not None:\n",
    "    data_2 = pickle.load(open(DATA_PATH + f'all_data_{SECOND_DATASET}.pickle', 'rb'))\n",
    "    X_test_2, y_test_2 = find.get_data_matrix(data_2['text_test'], word2index, MAXLEN), data_2['y_test']\n",
    "    gender_test_2 = data_2['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_2, y_test_2, gender_test_2 = None, None, None\n",
    "\n",
    "# 1.3 Third dataset\n",
    "if THIRD_DATASET is not None:\n",
    "    data_3 = pickle.load(open(DATA_PATH + f'all_data_{THIRD_DATASET}.pickle', 'rb'))\n",
    "    X_test_3, y_test_3 = find.get_data_matrix(data_3['text_test'], word2index, MAXLEN), data_3['y_test']\n",
    "    gender_test_3 = data_3['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_3, y_test_3, gender_test_2  = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the result directory\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "result_folder = MAIN_DATASET + '_' + MODEL_ARCH + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '/'\n",
    "result_path = MODEL_PATH + result_folder\n",
    "os.mkdir(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     120000600   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 10)      6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 148, 10)      9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 147, 10)      12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_1 (MaskedDense)    (None, 2)            122         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 27,092\n",
      "Non-trainable params: 120,000,660\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a model\n",
    "if MODEL_ARCH == 'CNN':\n",
    "    model = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, FILTERS)\n",
    "else:\n",
    "    assert False, f\"Unsupported model architecture: {MODEL_ARCH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 3832 samples, validate on 1277 samples\n",
      "Epoch 1/300\n",
      " - 1s - loss: 0.6753 - acc: 0.6931 - val_loss: 0.2848 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28477, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 2/300\n",
      " - 0s - loss: 0.1922 - acc: 0.9324 - val_loss: 0.1528 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28477 to 0.15276, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 3/300\n",
      " - 0s - loss: 0.1255 - acc: 0.9564 - val_loss: 0.1264 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15276 to 0.12636, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 4/300\n",
      " - 0s - loss: 0.1023 - acc: 0.9650 - val_loss: 0.1165 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12636 to 0.11650, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 5/300\n",
      " - 0s - loss: 0.0870 - acc: 0.9723 - val_loss: 0.1088 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11650 to 0.10878, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 6/300\n",
      " - 0s - loss: 0.0755 - acc: 0.9776 - val_loss: 0.1044 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10878 to 0.10438, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 7/300\n",
      " - 0s - loss: 0.0665 - acc: 0.9809 - val_loss: 0.1038 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10438 to 0.10384, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 8/300\n",
      " - 0s - loss: 0.0587 - acc: 0.9854 - val_loss: 0.1012 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10384 to 0.10117, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 9/300\n",
      " - 0s - loss: 0.0518 - acc: 0.9880 - val_loss: 0.0968 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10117 to 0.09682, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 10/300\n",
      " - 0s - loss: 0.0459 - acc: 0.9909 - val_loss: 0.0955 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09682 to 0.09550, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 11/300\n",
      " - 0s - loss: 0.0408 - acc: 0.9937 - val_loss: 0.0956 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09550\n",
      "Epoch 12/300\n",
      " - 0s - loss: 0.0362 - acc: 0.9945 - val_loss: 0.0937 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09550 to 0.09374, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 13/300\n",
      " - 0s - loss: 0.0320 - acc: 0.9956 - val_loss: 0.0928 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09374 to 0.09279, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.0285 - acc: 0.9969 - val_loss: 0.0934 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09279\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.0254 - acc: 0.9979 - val_loss: 0.0920 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09279 to 0.09202, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.0226 - acc: 0.9982 - val_loss: 0.0915 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09202 to 0.09148, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.0204 - acc: 0.9992 - val_loss: 0.0913 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09148 to 0.09128, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN.h5\n",
      "Epoch 18/300\n",
      " - 0s - loss: 0.0181 - acc: 0.9995 - val_loss: 0.0922 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09128\n",
      "Epoch 19/300\n",
      " - 0s - loss: 0.0161 - acc: 0.9997 - val_loss: 0.0919 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09128\n",
      "Epoch 20/300\n",
      " - 0s - loss: 0.0144 - acc: 0.9997 - val_loss: 0.0931 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09128\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the model\n",
    "history = find.model_train(model, result_path + f'trained_{MODEL_ARCH}.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 722,\n",
      "                   'all_true': 731,\n",
      "                   'class_f1': 0.9607708189951822,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.9667590027700831,\n",
      "                   'class_recall': 0.9548563611491108,\n",
      "                   'true_positive': 698},\n",
      "               1: {'all_positive': 556,\n",
      "                   'all_true': 547,\n",
      "                   'class_f1': 0.9483227561196738,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.9406474820143885,\n",
      "                   'class_recall': 0.9561243144424132,\n",
      "                   'true_positive': 523}},\n",
      " 'total': {'accuracy': 0.9553990610328639,\n",
      "           'macro_f1': 0.9545959536911285,\n",
      "           'macro_precision': 0.9537032423922358,\n",
      "           'macro_recall': 0.955490337795762,\n",
      "           'micro_f1': 0.9553990610328639,\n",
      "           'micro_precision': 0.9553990610328639,\n",
      "           'micro_recall': 0.9553990610328639}}\n",
      "FPR = 0.04514363885088919\n",
      "FNR = 0.043875685557586835\n",
      "\n",
      "Female prediction:\n",
      "{'per_class': {0: {'all_positive': 84,\n",
      "                   'all_true': 101,\n",
      "                   'class_f1': 0.8324324324324324,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.9166666666666666,\n",
      "                   'class_recall': 0.7623762376237624,\n",
      "                   'true_positive': 77},\n",
      "               1: {'all_positive': 515,\n",
      "                   'all_true': 498,\n",
      "                   'class_f1': 0.9693978282329714,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.9533980582524272,\n",
      "                   'class_recall': 0.9859437751004017,\n",
      "                   'true_positive': 491}},\n",
      " 'total': {'accuracy': 0.9482470784641068,\n",
      "           'macro_f1': 0.9035721242277424,\n",
      "           'macro_precision': 0.9350323624595469,\n",
      "           'macro_recall': 0.874160006362082,\n",
      "           'micro_f1': 0.9482470784641068,\n",
      "           'micro_precision': 0.9482470784641068,\n",
      "           'micro_recall': 0.9482470784641068}}\n",
      "FPR = 0.2376237623762376\n",
      "FNR = 0.014056224899598393\n",
      "\n",
      "Male prediction:\n",
      "{'per_class': {0: {'all_positive': 638,\n",
      "                   'all_true': 630,\n",
      "                   'class_f1': 0.9794952681388014,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.9733542319749217,\n",
      "                   'class_recall': 0.9857142857142858,\n",
      "                   'true_positive': 621},\n",
      "               1: {'all_positive': 41,\n",
      "                   'all_true': 49,\n",
      "                   'class_f1': 0.711111111111111,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.7804878048780488,\n",
      "                   'class_recall': 0.6530612244897959,\n",
      "                   'true_positive': 32}},\n",
      " 'total': {'accuracy': 0.9617083946980854,\n",
      "           'macro_f1': 0.8471787164027066,\n",
      "           'macro_precision': 0.8769210184264853,\n",
      "           'macro_recall': 0.8193877551020408,\n",
      "           'micro_f1': 0.9617083946980854,\n",
      "           'micro_precision': 0.9617083946980854,\n",
      "           'micro_recall': 0.9617083946980854}}\n",
      "FPR = 0.014285714285714285\n",
      "FNR = 0.3469387755102041\n",
      "----------------------------------------------------\n",
      "FPED = 0.22333804809052335\n",
      "FNED = 0.3328825506106057\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'original')\n",
    "else:\n",
    "    find.evaluate_all_gender(model, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model understanding and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedded_text_input (InputLayer (None, 150, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 149, 10)      6010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 10)      9010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 147, 10)      12010       embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 10)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 27,030\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,030\n",
      "__________________________________________________________________________________________________\n",
      "Num batches: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 35.68it/s]\n",
      " 20%|██        | 6/30 [00:01<00:07,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot summarize different ngrams ['seth g.s.', 'administrator of', 'administrator of'], 0.800849050283432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:02<00:06,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot summarize different ngrams ['and corrine', 'in lawsuits', 'in lawsuits'], 1.4616502523422241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:05<00:03,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot summarize different ngrams ['prior to becoming', 'prior to becoming', 'gretchen is in', 'prior to becoming', 'prior to becoming'], 0.5958486944437027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.36it/s]\n",
      "100%|██████████| 30/30 [00:05<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate wordclouds\n",
    "settings = {\n",
    "    'model_arch': MODEL_ARCH,\n",
    "    'filters': FILTERS,\n",
    "    'maxlen': MAXLEN,\n",
    "    'result_path': result_path,\n",
    "    'index2word': index2word,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "all_wordclouds = find.generate_wordclouds(model, X_train_1, settings, max_examples = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get input from a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_feature_enabled = [True for i in range(find.num_all_filters(FILTERS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI components from ipywidgets\n",
    "import ipywidgets as wgt\n",
    "\n",
    "def update_screen(feature_idx):\n",
    "    show_action_panel(feature_idx)\n",
    "    wordcloud = all_wordclouds[feature_idx]\n",
    "    f, ax = plt.subplots()\n",
    "    plt.rcParams['figure.figsize'] = [14, 7]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    W = model.layers[-1].get_weights()[0] # For the final layer\n",
    "    weight_plot = find.visualize_weights(W, feature_idx, class_names, show = False)\n",
    "    plt.show()\n",
    "\n",
    "def update_action(action):\n",
    "    global feature_radio_button, is_feature_enabled\n",
    "    feature_idx = feature_radio_button.value\n",
    "    if action == 'enabled':\n",
    "        print('enable')\n",
    "        is_feature_enabled[feature_idx] = True\n",
    "    elif action == 'disabled':\n",
    "        print('disable')\n",
    "        is_feature_enabled[feature_idx] = False\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "def show_action_panel(feature_idx):\n",
    "    global action_radio_button\n",
    "    action_radio_button.description = f'Current status of feature {feature_idx}:'\n",
    "    action_radio_button.value = 'enabled' if is_feature_enabled[feature_idx] else 'disabled'\n",
    "    \n",
    "feature_radio_button = wgt.RadioButtons(options=list(range(30)), value=0, description='Feature:', disabled=False)\n",
    "action_radio_button = wgt.RadioButtons(options=['enabled', 'disabled'],\n",
    "    value = 'enabled' if is_feature_enabled[feature_radio_button.value] else 'disabled',\n",
    "    description = f'Current status of feature {feature_radio_button.value}:',\n",
    "    style = {'description_width': 'initial'},\n",
    "    disabled = False\n",
    ")\n",
    "\n",
    "wgt.interactive_output(update_action, {'action':action_radio_button})\n",
    "out = wgt.interactive_output(update_screen, {'feature_idx':feature_radio_button})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f26335762c43eda46c48566fed4f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Get input from a human \n",
    "# Please investigate word clouds of these features and disable some irrelevant features using the radio-buttons under the bar plot.\n",
    "# In particular, to reduce the model's bias, we should disable the features which detect gender information rather than occupation.\n",
    "# Once you are happy, please then proceed to the next cell.\n",
    "display(wgt.HBox([feature_radio_button, wgt.VBox([out, action_radio_button])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 30 features \n",
      "Enabled: 19 features \n",
      "Disabled: 11 features\n",
      "Disabled features: [0, 3, 4, 5, 8, 9, 13, 17, 18, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {len(is_feature_enabled)} features \\nEnabled: {sum(is_feature_enabled)} features \\nDisabled: {len(is_feature_enabled)-sum(is_feature_enabled)} features\")\n",
    "print(f\"Disabled features: {[i for i,s in enumerate(is_feature_enabled) if not s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and fine-tuning an improved classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 150, 300)     120000600   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 149, 10)      6010        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 148, 10)      9010        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 147, 10)      12010       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 10)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 10)           0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 10)           0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 30)           0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_3 (MaskedDense)    (None, 2)            122         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 62\n",
      "Non-trainable params: 120,027,690\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8. Create an improved model\n",
    "# 8.1 Copy the existing CNN features\n",
    "model_improved = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, \n",
    "                                    FILTERS, trainable_filters = False)\n",
    "model_improved.set_weights(model.get_weights()) \n",
    "\n",
    "# 8.2 Apply human decisions to disable irrelevant features\n",
    "for idx, enable in enumerate(is_feature_enabled):\n",
    "    if not enable:\n",
    "        model_improved.layers[-1].disable_mask(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3832 samples, validate on 1277 samples\n",
      "Epoch 1/300\n",
      " - 0s - loss: 0.1082 - acc: 0.9564 - val_loss: 0.1360 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13604, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 2/300\n",
      " - 0s - loss: 0.0617 - acc: 0.9919 - val_loss: 0.1193 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13604 to 0.11925, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 3/300\n",
      " - 0s - loss: 0.0569 - acc: 0.9940 - val_loss: 0.1182 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11925 to 0.11820, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 4/300\n",
      " - 0s - loss: 0.0543 - acc: 0.9943 - val_loss: 0.1172 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11820 to 0.11718, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 5/300\n",
      " - 0s - loss: 0.0518 - acc: 0.9943 - val_loss: 0.1164 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11718 to 0.11636, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 6/300\n",
      " - 0s - loss: 0.0494 - acc: 0.9945 - val_loss: 0.1151 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11636 to 0.11512, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 7/300\n",
      " - 0s - loss: 0.0471 - acc: 0.9948 - val_loss: 0.1147 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11512 to 0.11470, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 8/300\n",
      " - 0s - loss: 0.0451 - acc: 0.9950 - val_loss: 0.1137 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11470 to 0.11371, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 9/300\n",
      " - 0s - loss: 0.0431 - acc: 0.9950 - val_loss: 0.1136 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11371 to 0.11356, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 10/300\n",
      " - 0s - loss: 0.0414 - acc: 0.9948 - val_loss: 0.1132 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11356 to 0.11322, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 11/300\n",
      " - 0s - loss: 0.0399 - acc: 0.9953 - val_loss: 0.1128 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11322 to 0.11283, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 12/300\n",
      " - 0s - loss: 0.0383 - acc: 0.9950 - val_loss: 0.1126 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11283 to 0.11260, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 13/300\n",
      " - 0s - loss: 0.0369 - acc: 0.9953 - val_loss: 0.1122 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11260 to 0.11216, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.0356 - acc: 0.9953 - val_loss: 0.1119 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11216 to 0.11191, saving model to trained_models/Biosbias2_CNN_20220522173805/trained_CNN_improved.h5\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.0344 - acc: 0.9953 - val_loss: 0.1121 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11191\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.0333 - acc: 0.9953 - val_loss: 0.1120 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11191\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.0323 - acc: 0.9956 - val_loss: 0.1119 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11191\n"
     ]
    }
   ],
   "source": [
    "# 9. Fine-tuning the improved model\n",
    "history = find.model_train(model_improved, result_path + f'trained_{MODEL_ARCH}_improved.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 717,\n",
      "                   'all_true': 731,\n",
      "                   'class_f1': 0.9530386740331491,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.9623430962343096,\n",
      "                   'class_recall': 0.9439124487004104,\n",
      "                   'true_positive': 690},\n",
      "               1: {'all_positive': 561,\n",
      "                   'all_true': 547,\n",
      "                   'class_f1': 0.9386281588447652,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.9269162210338681,\n",
      "                   'class_recall': 0.9506398537477148,\n",
      "                   'true_positive': 520}},\n",
      " 'total': {'accuracy': 0.94679186228482,\n",
      "           'macro_f1': 0.9459510539058925,\n",
      "           'macro_precision': 0.9446296586340888,\n",
      "           'macro_recall': 0.9472761512240626,\n",
      "           'micro_f1': 0.94679186228482,\n",
      "           'micro_precision': 0.94679186228482,\n",
      "           'micro_recall': 0.94679186228482}}\n",
      "FPR = 0.0560875512995896\n",
      "FNR = 0.04936014625228519\n",
      "\n",
      "Female prediction:\n",
      "{'per_class': {0: {'all_positive': 99,\n",
      "                   'all_true': 101,\n",
      "                   'class_f1': 0.8200000000000001,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.8282828282828283,\n",
      "                   'class_recall': 0.8118811881188119,\n",
      "                   'true_positive': 82},\n",
      "               1: {'all_positive': 500,\n",
      "                   'all_true': 498,\n",
      "                   'class_f1': 0.9639278557114228,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.962,\n",
      "                   'class_recall': 0.9658634538152611,\n",
      "                   'true_positive': 481}},\n",
      " 'total': {'accuracy': 0.9398998330550918,\n",
      "           'macro_f1': 0.8919958526364431,\n",
      "           'macro_precision': 0.8951414141414141,\n",
      "           'macro_recall': 0.8888723209670365,\n",
      "           'micro_f1': 0.9398998330550918,\n",
      "           'micro_precision': 0.9398998330550918,\n",
      "           'micro_recall': 0.9398998330550918}}\n",
      "FPR = 0.18811881188118812\n",
      "FNR = 0.03413654618473896\n",
      "\n",
      "Male prediction:\n",
      "{'per_class': {0: {'all_positive': 618,\n",
      "                   'all_true': 630,\n",
      "                   'class_f1': 0.9743589743589743,\n",
      "                   'class_name': 'surgeon',\n",
      "                   'class_precision': 0.9838187702265372,\n",
      "                   'class_recall': 0.9650793650793651,\n",
      "                   'true_positive': 608},\n",
      "               1: {'all_positive': 61,\n",
      "                   'all_true': 49,\n",
      "                   'class_f1': 0.7090909090909091,\n",
      "                   'class_name': 'nurse',\n",
      "                   'class_precision': 0.639344262295082,\n",
      "                   'class_recall': 0.7959183673469388,\n",
      "                   'true_positive': 39}},\n",
      " 'total': {'accuracy': 0.9528718703976435,\n",
      "           'macro_f1': 0.8446367114810398,\n",
      "           'macro_precision': 0.8115815162608095,\n",
      "           'macro_recall': 0.8804988662131519,\n",
      "           'micro_f1': 0.9528718703976435,\n",
      "           'micro_precision': 0.9528718703976435,\n",
      "           'micro_recall': 0.9528718703976435}}\n",
      "FPR = 0.03492063492063492\n",
      "FNR = 0.20408163265306123\n",
      "----------------------------------------------------\n",
      "FPED = 0.1531981769605532\n",
      "FNED = 0.1699450864683223\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluate the improved model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'debugged')\n",
    "else:\n",
    "    find.evaluate_all_gender(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'debugged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
