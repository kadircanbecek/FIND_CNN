{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging 20Newsgroups\n",
    "- **Task**: Classify \"Christianity\" vs \"Atheism\" documents from the 20 Newsgroups dataset.\n",
    "- **Problem**: The 20Newsgroups dataset is special because it contains a lot of artifacts – tokens (e.g., person names, punctuation marks) which are not relevant, but strongly cooccur with one of the classes. For evaluation, we therefore used the Religion dataset by [Ribeiro et al. (2016)](https://arxiv.org/pdf/1602.04938.pdf), containing \"Christianity\" and \"Atheism\" web pages, as a target dataset.\n",
    "- **Solution**: We use our framework to identify the features detecting irrelevant words (that do not capture the meaning of Christianity/Atheism and cannot generalize to the Religion dataset) and disable such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Notebook setup\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = InteractiveSession(config=config)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set random seed to create reproducable results\n",
    "the_seed = 1234\n",
    "np.random.seed(the_seed)\n",
    "random.seed(the_seed)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(the_seed)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe word embeddings: Please replace the string in the second line with a path to your GloVe embeddings file which can be download [here](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_PATH = f\"GLoVe/glove.6B.{EMBEDDING_DIM}d.txt\" # Path to your glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'EMNLP2020-Data2Share/Data2Share/' \n",
    "MAIN_DATASET = 'Wikitoxic'\n",
    "SECOND_DATASET = None\n",
    "THIRD_DATASET = None\n",
    "GENDER_BIAS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'trained_models/' # Path to save your trained models\n",
    "MODEL_ARCH = 'CNN'\n",
    "MAXLEN = 150\n",
    "FILTERS = [(10, 2), (10, 3), (10, 4)] # Ten filters of each window size [2,3,4]\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14605.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# 0. Load GloVe embeddings\n",
    "embedding_matrix, vocab_size, index2word, word2index = find.get_embedding_matrix(EMBEDDING_PATH, EMBEDDING_DIM, pad_initialisation = \"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56958/56958 [00:24<00:00, 2352.46it/s]\n",
      "100%|██████████| 19111/19111 [00:06<00:00, 3031.53it/s]\n",
      "100%|██████████| 18965/18965 [00:06<00:00, 3019.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load datasets and prepare inputs\n",
    "# 1.1 Main dataset\n",
    "data_1 = pickle.load(open(DATA_PATH + f'all_data_{MAIN_DATASET}.pickle', 'rb'))\n",
    "class_names = data_1['class_names']\n",
    "X_train_1, X_validate_1, X_test_1 = find.get_data_matrix(data_1['text_train'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_validate'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_test'], word2index, MAXLEN)\n",
    "y_test_1 = data_1['y_test']\n",
    "gender_test_1 = data_1['gender_test'] if GENDER_BIAS else None\n",
    "\n",
    "# 1.2 Second dataset\n",
    "if SECOND_DATASET is not None:\n",
    "    data_2 = pickle.load(open(DATA_PATH + f'all_data_{SECOND_DATASET}.pickle', 'rb'))\n",
    "    X_test_2, y_test_2 = find.get_data_matrix(data_2['text_test'], word2index, MAXLEN), data_2['y_test']\n",
    "    gender_test_2 = data_2['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_2, y_test_2, gender_test_2 = None, None, None\n",
    "\n",
    "# 1.3 Third dataset\n",
    "if THIRD_DATASET is not None:\n",
    "    data_3 = pickle.load(open(DATA_PATH + f'all_data_{THIRD_DATASET}.pickle', 'rb'))\n",
    "    X_test_3, y_test_3 = find.get_data_matrix(data_3['text_test'], word2index, MAXLEN), data_3['y_test']\n",
    "    gender_test_3 = data_3['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_3, y_test_3, gender_test_2  = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the result directory\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "result_folder = MAIN_DATASET + '_' + MODEL_ARCH + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '/'\n",
    "result_path = MODEL_PATH + result_folder\n",
    "os.mkdir(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     120000600   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 10)      6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 148, 10)      9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 147, 10)      12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_1 (MaskedDense)    (None, 2)            122         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 27,092\n",
      "Non-trainable params: 120,000,660\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a model\n",
    "if MODEL_ARCH == 'CNN':\n",
    "    model = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, FILTERS)\n",
    "else:\n",
    "    assert False, f\"Unsupported model architecture: {MODEL_ARCH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kanspretsa/miniconda3/envs/find-nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 56958 samples, validate on 19111 samples\n",
      "Epoch 1/300\n",
      " - 4s - loss: 0.0963 - acc: 0.9671 - val_loss: 0.0269 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02690, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN.h5\n",
      "Epoch 2/300\n",
      " - 3s - loss: 0.0217 - acc: 0.9932 - val_loss: 0.0206 - val_acc: 0.9935\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02690 to 0.02057, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN.h5\n",
      "Epoch 3/300\n",
      " - 3s - loss: 0.0151 - acc: 0.9955 - val_loss: 0.0174 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02057 to 0.01738, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN.h5\n",
      "Epoch 4/300\n",
      " - 3s - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0160 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01738 to 0.01595, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN.h5\n",
      "Epoch 5/300\n",
      " - 3s - loss: 0.0081 - acc: 0.9980 - val_loss: 0.0164 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01595\n",
      "Epoch 6/300\n",
      " - 3s - loss: 0.0058 - acc: 0.9985 - val_loss: 0.0167 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01595\n",
      "Epoch 7/300\n",
      " - 3s - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0156 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01595 to 0.01564, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN.h5\n",
      "Epoch 8/300\n",
      " - 3s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0167 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01564\n",
      "Epoch 9/300\n",
      " - 3s - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0161 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01564\n",
      "Epoch 10/300\n",
      " - 3s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0165 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01564\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the model\n",
    "history = find.model_train(model, result_path + f'trained_{MODEL_ARCH}.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 18074,\n",
      "                   'all_true': 18064,\n",
      "                   'class_f1': 0.9971774863025071,\n",
      "                   'class_name': 'Not abusive',\n",
      "                   'class_precision': 0.9969016266460109,\n",
      "                   'class_recall': 0.9974534986713907,\n",
      "                   'true_positive': 18018},\n",
      "               1: {'all_positive': 891,\n",
      "                   'all_true': 901,\n",
      "                   'class_f1': 0.9430803571428572,\n",
      "                   'class_name': 'Abusive',\n",
      "                   'class_precision': 0.9483726150392817,\n",
      "                   'class_recall': 0.9378468368479467,\n",
      "                   'true_positive': 845}},\n",
      " 'total': {'accuracy': 0.9946216715001318,\n",
      "           'macro_f1': 0.9701372355334461,\n",
      "           'macro_precision': 0.9726371208426463,\n",
      "           'macro_recall': 0.9676501677596687,\n",
      "           'micro_f1': 0.9946216715001318,\n",
      "           'micro_precision': 0.9946216715001318,\n",
      "           'micro_recall': 0.9946216715001318}}\n",
      "FPR = 0.002546501328609389\n",
      "FNR = 0.06215316315205328\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'original')\n",
    "else:\n",
    "    find.evaluate_all_gender(model, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model understanding and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedded_text_input (InputLayer (None, 150, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 149, 10)      6010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 10)      9010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 147, 10)      12010       embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 10)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 27,030\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,030\n",
      "__________________________________________________________________________________________________\n",
      "Num batches: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 16.97it/s]\n",
      "100%|██████████| 30/30 [00:08<00:00,  3.42it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate wordclouds\n",
    "settings = {\n",
    "    'model_arch': MODEL_ARCH,\n",
    "    'filters': FILTERS,\n",
    "    'maxlen': MAXLEN,\n",
    "    'result_path': result_path,\n",
    "    'index2word': index2word,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "all_wordclouds = find.generate_wordclouds(model, X_train_1, settings, max_examples = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get input from a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_feature_enabled = [True for i in range(find.num_all_filters(FILTERS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI components from ipywidgets\n",
    "import ipywidgets as wgt\n",
    "\n",
    "def update_screen(feature_idx):\n",
    "    show_action_panel(feature_idx)\n",
    "    wordcloud = all_wordclouds[feature_idx]\n",
    "    f, ax = plt.subplots()\n",
    "    plt.rcParams['figure.figsize'] = [14, 7]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    W = model.layers[-1].get_weights()[0] # For the final layer\n",
    "    weight_plot = find.visualize_weights(W, feature_idx, class_names, show = False)\n",
    "    plt.show()\n",
    "\n",
    "def update_action(action):\n",
    "    global feature_radio_button, is_feature_enabled\n",
    "    feature_idx = feature_radio_button.value\n",
    "    if action == 'enabled':\n",
    "        print('enable')\n",
    "        is_feature_enabled[feature_idx] = True\n",
    "    elif action == 'disabled':\n",
    "        print('disable')\n",
    "        is_feature_enabled[feature_idx] = False\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "def show_action_panel(feature_idx):\n",
    "    global action_radio_button\n",
    "    action_radio_button.description = f'Current status of feature {feature_idx}:'\n",
    "    action_radio_button.value = 'enabled' if is_feature_enabled[feature_idx] else 'disabled'\n",
    "    \n",
    "feature_radio_button = wgt.RadioButtons(options=list(range(30)), value=0, description='Feature:', disabled=False)\n",
    "action_radio_button = wgt.RadioButtons(options=['enabled', 'disabled'],\n",
    "    value = 'enabled' if is_feature_enabled[feature_radio_button.value] else 'disabled',\n",
    "    description = f'Current status of feature {feature_radio_button.value}:',\n",
    "    style = {'description_width': 'initial'},\n",
    "    disabled = False\n",
    ")\n",
    "\n",
    "wgt.interactive_output(update_action, {'action':action_radio_button})\n",
    "out = wgt.interactive_output(update_screen, {'feature_idx':feature_radio_button})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2843eb1b93b6413280d411fff04ec7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Get input from a human \n",
    "# Please investigate word clouds of these features and disable some irrelevant features using the radio-buttons under the bar plot. \n",
    "# Once you are happy, please then proceed to the next cell.\n",
    "display(wgt.HBox([feature_radio_button, wgt.VBox([out, action_radio_button])]))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 30 features \n",
      "Enabled: 25 features \n",
      "Disabled: 5 features\n",
      "Disabled features: [3, 5, 23, 25, 28]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {len(is_feature_enabled)} features \\nEnabled: {sum(is_feature_enabled)} features \\nDisabled: {len(is_feature_enabled)-sum(is_feature_enabled)} features\")\n",
    "print(f\"Disabled features: {[i for i,s in enumerate(is_feature_enabled) if not s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and fine-tuning an improved classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 300)     120000600   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 149, 10)      6010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 148, 10)      9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 147, 10)      12010       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 10)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 10)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 10)           0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 30)           0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_2 (MaskedDense)    (None, 2)            122         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 62\n",
      "Non-trainable params: 120,027,690\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8. Create an improved model\n",
    "# 8.1 Copy the existing CNN features\n",
    "model_improved = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, \n",
    "                                    FILTERS, trainable_filters = False)\n",
    "model_improved.set_weights(model.get_weights()) \n",
    "\n",
    "# 8.2 Apply human decisions to disable irrelevant features\n",
    "for idx, enable in enumerate(is_feature_enabled):\n",
    "    if not enable:\n",
    "        model_improved.layers[-1].disable_mask(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56958 samples, validate on 19111 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0158 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01578, saving model to trained_models/Wikitoxic_CNN_20220522194909/trained_CNN_improved.h5\n",
      "Epoch 2/300\n",
      " - 2s - loss: 0.0026 - acc: 0.9996 - val_loss: 0.0175 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01578\n",
      "Epoch 3/300\n",
      " - 2s - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0170 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01578\n",
      "Epoch 4/300\n",
      " - 2s - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0180 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01578\n"
     ]
    }
   ],
   "source": [
    "# 9. Fine-tuning the improved model\n",
    "history = find.model_train(model_improved, result_path + f'trained_{MODEL_ARCH}_improved.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 18083,\n",
      "                   'all_true': 18064,\n",
      "                   'class_f1': 0.9972058538744571,\n",
      "                   'class_name': 'Not abusive',\n",
      "                   'class_precision': 0.9966819664878616,\n",
      "                   'class_recall': 0.9977302922940655,\n",
      "                   'true_positive': 18023},\n",
      "               1: {'all_positive': 882,\n",
      "                   'all_true': 901,\n",
      "                   'class_f1': 0.9433538979248458,\n",
      "                   'class_name': 'Abusive',\n",
      "                   'class_precision': 0.953514739229025,\n",
      "                   'class_recall': 0.9334073251942286,\n",
      "                   'true_positive': 841}},\n",
      " 'total': {'accuracy': 0.9946744002109148,\n",
      "           'macro_f1': 0.970310183638499,\n",
      "           'macro_precision': 0.9750983528584433,\n",
      "           'macro_recall': 0.9655688087441471,\n",
      "           'micro_f1': 0.9946744002109148,\n",
      "           'micro_precision': 0.9946744002109148,\n",
      "           'micro_recall': 0.9946744002109148}}\n",
      "FPR = 0.002269707705934455\n",
      "FNR = 0.06659267480577137\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluate the improved model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'debugged')\n",
    "else:\n",
    "    find.evaluate_all_gender(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'debugged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
